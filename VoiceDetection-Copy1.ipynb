{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61ce3014-9bf7-4dc0-95ef-e42a3cd033af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\21ste\\anaconda3\\envs\\audioclassification\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\21ste\\anaconda3\\envs\\audioclassification\\lib\\site-packages (0.19.1)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\21ste\\anaconda3\\envs\\audioclassification\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\21ste\\anaconda3\\envs\\audioclassification\\lib\\site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\21ste\\anaconda3\\envs\\audioclassification\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\21ste\\anaconda3\\envs\\audioclassification\\lib\\site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\21ste\\anaconda3\\envs\\audioclassification\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\21ste\\anaconda3\\envs\\audioclassification\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\21ste\\anaconda3\\envs\\audioclassification\\lib\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\21ste\\anaconda3\\envs\\audioclassification\\lib\\site-packages (from torchvision) (2.1.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\21ste\\anaconda3\\envs\\audioclassification\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\21ste\\anaconda3\\envs\\audioclassification\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\21ste\\anaconda3\\envs\\audioclassification\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3418b14d-3902-42c2-a4ab-4e90a0abf9ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "540c88e3-b1a1-4e08-86ee-640f47f8ce18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "path = os.path.join(r'C:\\Users\\21ste\\Downloads\\AudioClass\\audio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7270375e-06da-4c64-80da-b2b10b7f4ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "class AugmentedAudioDataset(Dataset):\n",
    "    def __init__(self, audio_dir, target_num=48000, transform=None, sample_rate=16000, n_mels=64):\n",
    "        self.audio_dir = audio_dir\n",
    "        self.target_num_samples = sample_rate * 3          # 16000 (sample_rate) * 3 (seconds) = 48000\n",
    "        self.audio_files = [f for f in os.listdir(audio_dir) if f.endswith('.wav')]\n",
    "        self.transform = transform\n",
    "        self.sample_rate = sample_rate\n",
    "        self.mel_transform = T.MelSpectrogram(sample_rate=self.sample_rate, n_mels=n_mels)\n",
    "        self.noise = torch.randn(1, sample_rate) * 0.005  # Add some random noise\n",
    "\n",
    "    def _pad_or_truncate(self, waveform):\n",
    "        num_samples = waveform.shape[1]\n",
    "        \n",
    "        if num_samples > self.target_num_samples:\n",
    "            # Truncate if too long\n",
    "            waveform = waveform[:, :self.target_num_samples]\n",
    "        elif num_samples < self.target_num_samples:\n",
    "            # Pad with zeros if too short\n",
    "            padding = self.target_num_samples - num_samples\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
    "        \n",
    "        return waveform\n",
    "\n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        if sr != self.sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.sample_rate)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "\n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = os.path.join(self.audio_dir, self.audio_files[idx])\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "        '''\n",
    "        # Data augmentation\n",
    "        # 1. Add noise\n",
    "        if random.random() < 0.5:  # 50% chance to add noise\n",
    "            waveform += self.noise[:, :waveform.size(1)]\n",
    "\n",
    "        # 2. Random pitch shift\n",
    "        if random.random() < 0.5:\n",
    "            n_steps = random.randint(-3, 3)  # Shift pitch by -3 to +3 semitones\n",
    "            waveform = T.PitchShift(self.sample_rate, n_steps)(waveform)\n",
    "\n",
    "        # 3. Time stretching\n",
    "        if random.random() < 0.5:\n",
    "            stretch_factor = random.uniform(0.8, 1.2)  # Stretch between 80% and 120%\n",
    "            waveform = T.TimeStretch(stretch_factor)(waveform)\n",
    "\n",
    "        # 4. Random volume change\n",
    "        if random.random() < 0.5:\n",
    "            volume_factor = random.uniform(0.5, 1.5)  # Random volume change\n",
    "            waveform *= volume_factor\n",
    "        '''\n",
    "\n",
    "        waveform = self._resample_if_necessary(waveform, self.sample_rate)\n",
    "        #print(f\"resample wave = {waveform.shape}\")\n",
    "        waveform = self._mix_down_if_necessary(waveform)\n",
    "        waveform = self._pad_or_truncate(waveform)\n",
    "\n",
    "        mel_spec = self.mel_transform(waveform)  # Convert to Mel Spectrogram\n",
    "\n",
    "        # Apply any additional transforms (like resizing or augmentations)\n",
    "        if self.transform:\n",
    "            mel_spec = self.transform(mel_spec)\n",
    "            \n",
    "        mel_spec = mel_spec.squeeze()\n",
    "        # Label assignment (e.g., \"Hey Google\" = 1, \"Other\" = 0)\n",
    "        label = np.array([1, 0])  if 'output' in audio_path else np.array([0, 1])\n",
    "\n",
    "        return mel_spec, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "161a8dd1-9704-4ed2-bc14-1d28a74cbf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = AugmentedAudioDataset(path)\n",
    "train_loader = DataLoader(data, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cba03ee1-08c2-4207-8421-24233580f87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio.transforms as T\n",
    "\n",
    "# Transform raw audio into Mel Spectrogram (suitable for CNN input)\n",
    "def preprocess_audio(audio_waveform, sample_rate=16000, n_mels=64):\n",
    "    mel_spectrogram = T.MelSpectrogram(sample_rate=sample_rate, n_mels=n_mels)\n",
    "    return mel_spectrogram(audio_waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aeee4a36-9451-4861-b109-2a44a0e3d1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_size1, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_size1, hidden_size2, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(hidden_size2, hidden_size3, batch_first=True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_size3, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, output_size)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x, _ = self.lstm3(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Take only the output of the last time step\n",
    "        x = x[:, -1, :]\n",
    "        \n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.softmax(self.fc3(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Assuming the input shape is (30, 1662) and actions.shape[0] is the number of classes\n",
    "input_size = 241\n",
    "hidden_size1 = 64\n",
    "hidden_size2 = 128\n",
    "hidden_size3 = 64\n",
    "output_size = 1  # Replace this with the actual number of classes\n",
    "model = LSTMModel(input_size, hidden_size1, hidden_size2, hidden_size3, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "616124c8-9f86-4a93-aa5e-fa224c0997f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Adjust the Mel Spectrogram to fit MobileNetV2's input (3 channels)\n",
    "def mel_to_rgb_input(mel_spec):\n",
    "    # Repeat the Mel Spectrogram across 3 channels to simulate RGB input\n",
    "    mel_rgb = mel_spec.repeat(1, 3, 1, 1)\n",
    "    return mel_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f71f4111-0684-44a4-8d43-1ac5d6c4cb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 0.0796\n",
      "Epoch [2/15], Loss: 0.0295\n",
      "Epoch [3/15], Loss: 0.0098\n",
      "Epoch [4/15], Loss: 0.0523\n",
      "Epoch [5/15], Loss: 0.0531\n",
      "Epoch [6/15], Loss: 0.0531\n",
      "Epoch [7/15], Loss: 0.0528\n",
      "Epoch [8/15], Loss: 0.0531\n",
      "Epoch [9/15], Loss: 0.0530\n",
      "Epoch [10/15], Loss: 0.0528\n",
      "Epoch [11/15], Loss: 0.0529\n",
      "Epoch [12/15], Loss: 0.0530\n",
      "Epoch [13/15], Loss: 0.0529\n",
      "Epoch [14/15], Loss: 0.0529\n",
      "Epoch [15/15], Loss: 0.0529\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "num_epochs = 15\n",
    "\n",
    "# Initialize the model\n",
    "model.train()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):  # Number of epochs\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        labels = labels.float()\n",
    "\n",
    "        \n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        #print(outputs.shape)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss statistics\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "223d1ab1-e3b1-4aae-9867-fc93d0e7f3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fffd1a25-99cb-4967-a778-011873d7e89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the label map\n",
    "label_map = ['Detected Name', 'Random Sound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d2649604-2011-40ea-b05e-904dc6db1b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Skylar\n"
     ]
    }
   ],
   "source": [
    "# Testing our model using our data\n",
    "test_data = AugmentedAudioDataset(path)\n",
    "wave, label = test_data[1495]\n",
    "label = torch.from_numpy(label)\n",
    "print(label_map[torch.argmax(label).item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2cedb812-5bbf-44bc-8eff-fc62548de496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function that detects the wake word\n",
    "\n",
    "# Set to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Detect keywords (inference)\n",
    "def detect_keyword(quantized_model, audio_waveform, sample_rate=16000):\n",
    "    #mel_spec = preprocess_audio(audio_waveform, sample_rate)\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = quantized_model(audio_waveform.unsqueeze(0))\n",
    "        pred = torch.argmax(output).item()\n",
    "\n",
    "    if pred == 0:\n",
    "        print(\"Keyword detected: 'Hey Skylar'\")\n",
    "    else:\n",
    "        print(\"No keyword detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3fb6e6c4-5500-43d6-baa7-f41a766a8335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Skylar\n",
      "Keyword detected: 'Hey Skylar'\n"
     ]
    }
   ],
   "source": [
    "# Checking to see if the model is accurate\n",
    "test_data = AugmentedAudioDataset(path)\n",
    "wave, label = test_data[1485]\n",
    "label = torch.from_numpy(label)\n",
    "print(label_map[torch.argmax(label).item()])\n",
    "detect_keyword(model, wave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "61e7c510-1061-4989-940e-9f16ff80c197",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata = AugmentedAudioDataset(path)\n",
    "test_loader = DataLoader(testdata, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f8cd7bf-26b9-4e95-bda2-55272193f82b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0005143990856595337"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _mix_down_if_necessary(signal):\n",
    "    if signal.shape[0] > 1:\n",
    "        signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4f8a47a-9e4e-4317-bee4-4810232c8037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting for ambient noise... Please wait.\n",
      "Listening for live audio... Press Ctrl+C to stop.\n",
      "Recording...\n",
      "Recording...\n",
      "Recording...\n",
      "Recording...\n",
      "Recording...\n",
      "Recording...\n",
      "Keyword detected: 'Hey Skylar'\n",
      "Recording...\n",
      "Recording...\n",
      "Recording...\n",
      "Recording...\n",
      "Recording...\n",
      "Keyword detected: 'Hey Skylar'\n",
      "Recording...\n",
      "Keyword detected: 'Hey Skylar'\n",
      "Recording...\n",
      "Stopped listening.\n"
     ]
    }
   ],
   "source": [
    "# Live WakeWord detection\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import speech_recognition as sr\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "# Load your trained model (replace with the actual path to your model)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Define audio processing parameters\n",
    "sample_rate = 16000  # The sample rate used in your model training\n",
    "n_mels = 64  # Number of Mel bands (must match your training setup)\n",
    "target_length = 3 * sample_rate  # Target length (e.g., 3 seconds of audio)\n",
    "\n",
    "# Create Mel Spectrogram transform\n",
    "mel_transform = T.MelSpectrogram(sample_rate=sample_rate, n_mels=n_mels)\n",
    "\n",
    "# SpeechRecognition recognizer instance\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Function to process audio and make predictions\n",
    "def predict_on_audio(audio_data):\n",
    "    # Convert audio to a tensor and preprocess it\n",
    "    audio_tensor = torch.tensor(audio_data).unsqueeze(0).float()\n",
    "\n",
    "    # Normalize audio\n",
    "    audio_tensor = audio_tensor / audio_tensor.abs().max()\n",
    "\n",
    "    # Preprocess the audio: truncate or pad to the target length\n",
    "    if audio_tensor.size(1) > target_length:\n",
    "        audio_tensor = audio_tensor[:, :target_length]  # Truncate\n",
    "    elif audio_tensor.size(1) < target_length:\n",
    "        padding = torch.zeros(1, target_length - audio_tensor.size(1))\n",
    "        audio_tensor = torch.cat((audio_tensor, padding), dim=1)  # Pad\n",
    "\n",
    "    # Convert to Mel Spectrogram\n",
    "    mel_spec = mel_transform(audio_tensor)\n",
    "\n",
    "    # Add an extra dimension for batch size (1, n_mels, time)\n",
    "    mel_spec = mel_spec.unsqueeze(0)\n",
    "    mel_spec = mel_to_rgb_input(mel_spec)\n",
    "\n",
    "    # Make prediction using the model\n",
    "    with torch.no_grad():\n",
    "        output = model(mel_spec)\n",
    "        \n",
    "    if output.item() > 0.7:\n",
    "        print(\"Keyword detected\")\n",
    "\n",
    "# Function to capture live audio from the microphone and make predictions\n",
    "def live_audio_detection():\n",
    "    with sr.Microphone(sample_rate=sample_rate) as source:\n",
    "        print(\"Adjusting for ambient noise... Please wait.\")\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "\n",
    "        print(\"Listening for live audio... Press Ctrl+C to stop.\")\n",
    "        try:\n",
    "            while True:\n",
    "                print(\"Recording...\")\n",
    "                audio = recognizer.listen(source, phrase_time_limit=3)  # Capture 3 seconds of audio\n",
    "\n",
    "                # Convert the audio data to numpy array (raw PCM format)\n",
    "                audio_data = np.frombuffer(audio.get_raw_data(), dtype=np.int16).astype(np.float32)\n",
    "\n",
    "                # Make a prediction\n",
    "                predict_on_audio(audio_data)\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Stopped listening.\")\n",
    "\n",
    "# Start live audio detection\n",
    "live_audio_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d1c6d3-c6be-4e59-93dd-48600b8de1f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
